
# ğŸ§  Entendendo o Q-Learning no Jogo da Velha

Este documento explica a lÃ³gica matemÃ¡tica e prÃ¡tica por trÃ¡s da IA usada neste projeto de Jogo da Velha com Q-Learning.

---

## ğŸ¤– O que Ã© Q-Learning?

**Q-Learning** Ã© um algoritmo de aprendizado por reforÃ§o que permite a um agente aprender uma polÃ­tica Ã³tima de aÃ§Ãµes com base em tentativas, erros e recompensas.

---

## ğŸ§® A EquaÃ§Ã£o de AtualizaÃ§Ã£o

A fÃ³rmula usada para atualizar os valores Q Ã©:

```
Q(s, a) â† Q(s, a) + Î± * [ r + Î³ * max(Q(s', a')) - Q(s, a) ]
```

Onde:

- `s` â†’ estado atual
- `a` â†’ aÃ§Ã£o executada
- `r` â†’ recompensa recebida apÃ³s aÃ§Ã£o
- `s'` â†’ prÃ³ximo estado
- `Î±` (alpha) â†’ taxa de aprendizado
- `Î³` (gamma) â†’ fator de desconto do futuro
- `max Q(s', a')` â†’ melhor valor estimado da prÃ³xima jogada

---

## ğŸ§  Como isso Ã© aplicado no jogo

### Estados (`s`)
SÃ£o representaÃ§Ãµes do tabuleiro, como uma string `'XOX O X O'`.

### AÃ§Ãµes (`a`)
SÃ£o coordenadas disponÃ­veis no tabuleiro, como `(0, 2)`.

### Recompensas (`r`)
- VitÃ³ria: `+1`
- Derrota: `-1`
- Empate: `0`

---

## ğŸ² ExploraÃ§Ã£o vs ExploraÃ§Ã£o

A IA usa **Îµ-greedy** para decidir entre:

- **Explorar** jogadas novas: com probabilidade `Îµ`
- **Explorar** jogadas aprendidas: com probabilidade `1 - Îµ`

O valor de `Îµ` decai com o tempo:

```python
epsilon *= epsilon_decay  # atÃ© atingir epsilon_min
```

---

## ğŸ“Š ParÃ¢metros do agente

```python
alpha = 0.1        # aprendizado
gamma = 0.9        # desconto
epsilon = 0.9      # exploraÃ§Ã£o inicial
epsilon_decay = 0.995
epsilon_min = 0.1
```

---

## ğŸ“š Vantagens no Jogo da Velha

- EspaÃ§o de estados pequeno (~5 mil)
- IA pode treinar jogando contra si mesma
- Aprendizado rÃ¡pido e sem supervisÃ£o

---

## âœ… Resultado

ApÃ³s o treinamento, a IA aprende a jogar de forma competitiva contra humanos ou aleatÃ³rios e o modelo Ã© salvo em:

```
modelos/qlearning_model.pkl
```

---
